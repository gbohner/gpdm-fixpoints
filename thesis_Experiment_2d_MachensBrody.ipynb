{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machens-Brody 2D discrimination model\n",
    "\n",
    "We implement and simulate from the 2005 Machens-Romo-Brody 2 population spiking model (with the nullclines simulated via their mean field model). The spiking electrical activity is then simulated to be observed through a fictional non-saturating, fixed onset and decay time calcium reporter with parameters ...\n",
    "\n",
    "We are using the provided Matlab code to simulate the spiking dynamics, with slight changes to the simulation:\n",
    "- We simulate the mean field dynamics as is (Euler model with dt=1msec), but injecting additive noise at the beginning (par.init_Sigma) and also (par.noise_eps) every par.noiseTimeStep msec, as is in our model\n",
    "- The loading dataset uses the dynamics only when f1 is active, and the decision dataset is simulated when only f2 is active [both starting from (0,0) initial conditions]\n",
    "- Only ambigous trials, type [4,4] in the code\n",
    "\n",
    "See the parameter initialisations in ```thesis_initpar_loading.m``` and ```thesis_initpar_decision.m```, and the simulation process in ```spikesim_special.m``` and ```mfsim_special.m```. The datasets were created by ```thesis_create_ExperimentData_spiking.m```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.linalg\n",
    "import scipy.cluster\n",
    "import scipy.stats\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import iplot as plt\n",
    "from plotly import graph_objs as plt_type\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "\n",
    "# # Import 1d well examples\n",
    "from GPDM_direct_fixedpoints import *\n",
    "\n",
    "# plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing\n",
    "# import itertools\n",
    "\n",
    "# num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "import pickle\n",
    "import pickle, datetime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loading_uneven = scipy.io.loadmat('thesis_Machens_Brody_sim/thesis_MachensBrodySim_20181230T121236_loading_uneven.mat', squeeze_me=True)\n",
    "data_loading_even = scipy.io.loadmat('thesis_Machens_Brody_sim/thesis_MachensBrodySim_20181230T121236_loading_even.mat', squeeze_me=True)\n",
    "data_decision = scipy.io.loadmat('thesis_Machens_Brody_sim/thesis_MachensBrodySim_20181230T121236_decision.mat', squeeze_me=True)\n",
    "\n",
    "# data_loading_uneven = scipy.io.loadmat('thesis_Machens_Brody_sim/thesis_MachensBrodySim_20181129T172557_loading_uneven.mat', squeeze_me=True)\n",
    "# data_loading_even = scipy.io.loadmat('thesis_Machens_Brody_sim/thesis_MachensBrodySim_20181129T172557_loading_even.mat', squeeze_me=True)\n",
    "# data_decision = scipy.io.loadmat('thesis_Machens_Brody_sim/thesis_MachensBrodySim_20181129T172557_decision.mat', squeeze_me=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate calcium traces from the spike times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_curve(fred_params):\n",
    "    # Fast rise exponential decay model, A, tau1, tau2\n",
    "    t = 1+np.arange(999) # msec\n",
    "    out = ( \n",
    "        np.sqrt(np.exp(2*fred_params['tau1']/fred_params['tau2'])) * \n",
    "        np.exp(-fred_params['tau1']/t - t/fred_params['tau2'])\n",
    "    )\n",
    "    \n",
    "    t = np.concatenate([np.array([0]), t])\n",
    "    out = np.concatenate([np.array([0]), out])\n",
    "    \n",
    "    out = (fred_params['A']) * (out / np.max(out))\n",
    "     \n",
    "    return t, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params, load and resample the data\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "simParams = OrderedDict()\n",
    "simParams['seed'] = 1234\n",
    "simParams['curData'] = 'decision'  # decision or loading_even or loading_uneven \n",
    "if simParams['curData'] == \"loading_even\":\n",
    "    curData = data_loading_even\n",
    "elif simParams['curData'] == \"loading_uneven\":\n",
    "    curData = data_loading_uneven\n",
    "else:\n",
    "    curData = data_decision\n",
    "\n",
    "simParams['trial_nums'] = range(40)\n",
    "simParams['delta_t'] = 125 # msec sampling (8 Hz)\n",
    "simParams['T_max'] = 2001\n",
    "simParams['Sigma_calc_obs'] = 1e-2\n",
    "simParams['baseline_brightness'] = 1.\n",
    "simParams['fred_params'] = OrderedDict(A=0.19, tau1=45./2, tau2=142.) #OrderedDict(A=1, tau1=1, tau2=3.)#OrderedDict(A=1, tau1=6.5, tau2=120.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createData(curData, simParams, batchnum=0):\n",
    "    \n",
    "    # Setup the initial observations in y\n",
    "    obs_times = curData['data']['rt'][0][range(0, min(simParams['T_max'], curData['data']['rt'][0].shape[0]), simParams['delta_t'])]\n",
    "    \n",
    "    #y_spikes = []\n",
    "    \n",
    "    y_calcium = (\n",
    "        np.ones(((curData['par']['Nneurons']).astype(int)*2, len(obs_times), len(simParams['trial_nums'])))\n",
    "        * simParams['baseline_brightness']\n",
    "    )\n",
    "    \n",
    "    # Get the convultion kernel\n",
    "    conv_t, conv_weights = fred_curve(simParams['fred_params'])\n",
    "\n",
    "    # Gather data from each trial\n",
    "    for n_trial in simParams['trial_nums']:\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Go from spike times on single neurons to calcium dF/F type signals, sampled at low frequency\n",
    "        flourescence_cur_trial = np.zeros(((curData['par']['Nneurons']).astype(int)*2, len(curData['data']['rt'][0])))\n",
    "\n",
    "        # For each spike on each neuron, add the conv_weights appropriately\n",
    "        cur_spikes = np.concatenate([curData['data']['spikesx'][n_trial], curData['data']['spikesy'][n_trial]])\n",
    "        \n",
    "        for m_neuron in range(cur_spikes.shape[0]):\n",
    "            for s_spike in range(cur_spikes.shape[1]):\n",
    "                if cur_spikes[m_neuron,s_spike]==0: # No more spike times on that neuron\n",
    "                    break\n",
    "\n",
    "                time_ind = int(np.floor(cur_spikes[m_neuron,s_spike]))\n",
    "                flourescence_cur_trial[m_neuron, time_ind:min(time_ind+len(conv_weights),flourescence_cur_trial.shape[1])] += (\n",
    "                    conv_weights[0:(min(len(conv_weights),flourescence_cur_trial.shape[1]-time_ind))]\n",
    "                )\n",
    "            \n",
    "        # Questions: Mutliplicative? Point observation or time-gated sum or total cumulative sum?\n",
    "        # I think multiplicative point observations with added \"gaussian\" noise subsequently\n",
    "        y_calcium[:,:,n_trial] *= (1+flourescence_cur_trial[:, obs_times])\n",
    "        \n",
    "    # Add observation noise to y\n",
    "    y_calcium = y_calcium + np.sqrt(simParams['Sigma_calc_obs'])*np.random.randn(*y_calcium.shape)\n",
    "    \n",
    "    return y_calcium #, y_spikes # Add spikes output later (for Lea's code)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create various visualisations to describe the simulation process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default convolution curve\n",
    "visParams = copy.deepcopy(simParams)\n",
    "tmp = fred_curve(visParams['fred_params'])\n",
    "plt([plt_type.Scatter(x=tmp[0], y=tmp[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"binned spike time\" data\n",
    "visParams = copy.deepcopy(simParams)\n",
    "visParams['fred_params'] = OrderedDict(A=1, tau1=0.1, tau2=0.1)\n",
    "tmp = fred_curve(visParams['fred_params'])\n",
    "plt([plt_type.Scatter(x=tmp[0], y=tmp[1])])\n",
    "\n",
    "visParams['delta_t'] = 1\n",
    "visParams['Sigma_calc_obs'] = 0\n",
    "y = createData(curData, visParams, batchnum=0) - 1\n",
    "\n",
    "tmp_time_bin = 125 # ms\n",
    "tmp_obs_times = curData['data']['rt'][0][range(0, min(visParams['T_max'], curData['data']['rt'][0].shape[0]), tmp_time_bin)]\n",
    "    \n",
    "y = np.cumsum(y, axis=1)\n",
    "\n",
    "y = np.diff(y[:, tmp_obs_times, :], axis=1)\n",
    "\n",
    "# An example + and - neuron vs time on one trial \n",
    "all_plots = []\n",
    "\n",
    "for n_trial in [1]:\n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[:1, :, n_trial].mean(0)))\n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[-1:, :, n_trial].mean(0)))\n",
    "    #all_plots.append(plt_type.Scatter(x=y[:30,:, n_trial].mean(0), y=y[-30:,:, n_trial].mean(0)))\n",
    "    \n",
    "plt(all_plots)\n",
    "\n",
    "\n",
    "# An example + and - neuron vs time on one trial \n",
    "all_plots = []\n",
    "\n",
    "# Show population average for the two types of neurons, plus the individual traces\n",
    "\n",
    "for n_trial in [1]:\n",
    "    for n_neuron in range(30):\n",
    "        all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[n_neuron, :, n_trial], mode='lines', line=dict(color='gray', width=0.5)))\n",
    "    for n_neuron in range(30,60):\n",
    "        all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[n_neuron, :, n_trial], mode='lines', line=dict(color='gray', width=0.5)))\n",
    "    \n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[range(30), :, n_trial].mean(0), mode='lines', line=dict(color='blue')))\n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[range(30,60), :, n_trial].mean(0), mode='lines', line=dict(color='red')))\n",
    "\n",
    "    #all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[-1:, :, n_trial].mean(0)))\n",
    "    #all_plots.append(plt_type.Scatter(x=y[:30,:, n_trial].mean(0), y=y[-30:,:, n_trial].mean(0)))\n",
    "    \n",
    "plt(all_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with no noise\n",
    "visParams = copy.deepcopy(simParams)\n",
    "visParams['delta_t'] = 1\n",
    "visParams['Sigma_calc_obs'] = 0\n",
    "y = createData(curData, visParams, batchnum=0)\n",
    "\n",
    "# An example + and - neuron vs time on one trial \n",
    "all_plots = []\n",
    "\n",
    "for n_trial in [1]:\n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[:1, :, n_trial].mean(0)))\n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[-1:, :, n_trial].mean(0)))\n",
    "    #all_plots.append(plt_type.Scatter(x=y[:30,:, n_trial].mean(0), y=y[-30:,:, n_trial].mean(0)))\n",
    "    \n",
    "plt(all_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show typical noise level at 1 ms bins\n",
    "visParams = copy.deepcopy(simParams)\n",
    "y = createData(curData, visParams, batchnum=0)\n",
    "\n",
    "# An example + and - neuron vs time on one trial \n",
    "all_plots = []\n",
    "\n",
    "# Show population average for the two types of neurons, plus the individual traces\n",
    "\n",
    "for n_trial in [1]:\n",
    "    for n_neuron in range(30):\n",
    "        all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[n_neuron, :, n_trial], mode='lines', line=dict(color='gray', width=0.5)))\n",
    "    for n_neuron in range(30,60):\n",
    "        all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[n_neuron, :, n_trial], mode='lines', line=dict(color='gray', width=0.5)))\n",
    "    \n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[range(30), :, n_trial].mean(0), mode='lines', line=dict(color='blue')))\n",
    "    all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[range(30,60), :, n_trial].mean(0), mode='lines', line=dict(color='red')))\n",
    "\n",
    "    #all_plots.append(plt_type.Scatter(x=np.arange(y.shape[1]), y = y[-1:, :, n_trial].mean(0)))\n",
    "    #all_plots.append(plt_type.Scatter(x=y[:30,:, n_trial].mean(0), y=y[-30:,:, n_trial].mean(0)))\n",
    "    \n",
    "plt(all_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset used for fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = createData(curData, simParams, batchnum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example + vs - neuron on each trial\n",
    "all_plots = []\n",
    "for n_trial in range(y.shape[2]):\n",
    "    all_plots.append(plt_type.Scatter(x=y[:1,:, n_trial].mean(0), y=y[-1:,:, n_trial].mean(0)))\n",
    "    #all_plots.append(plt_type.Scatter(x=y[:30,:, n_trial].mean(0), y=y[-30:,:, n_trial].mean(0)))\n",
    "    \n",
    "plt(all_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the nullclines and the data\n",
    "plt(plt_type.Figure(data=\n",
    "    [plt_type.Scatter(x=curData['nullcline1'][0,:], y=curData['nullcline1'][1,:]),\n",
    "     plt_type.Scatter(x=curData['nullcline2'][0,:], y=curData['nullcline2'][1,:])\n",
    "    ], \n",
    "                    layout=plt_type.Layout(xaxis={'range': [0.,8.]}, yaxis={'range': [0.,8.]})))\n",
    "\n",
    "plots_by_run = []\n",
    "for v in range(y.shape[2]):\n",
    "    plots_by_run.append(\n",
    "        plt_type.Scatter(x=np.squeeze(y[0,:,v]), \n",
    "                      y=np.squeeze(y[-1,:,v]), \n",
    "                      mode='lines')\n",
    "    )\n",
    "    \n",
    "plt(plots_by_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a plotting function for callback that shows the current transition function estimate\n",
    "import plotly.figure_factory\n",
    "\n",
    "def machens_callback_plot_external(pvec_partial, \n",
    "                                  opt_params, init_paramvec, transforms, dict_ind, dict_shape, nullclines=None\n",
    "                                 ):\n",
    "    \n",
    "    paramvec = replace_params(pvec_partial, opt_params, init_paramvec)\n",
    "    paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "       \n",
    "    # Unpack the usual parameters\n",
    "    (Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J)  = \\\n",
    "        list(paramdict.values())[:12]\n",
    "    \n",
    "    if np.any(np.isnan(lengthscales)):\n",
    "        set_trace()\n",
    "    \n",
    "    # Deal with the extra possible parameters\n",
    "    Sigma_s = None; Sigma_J=None;\n",
    "    if 'Sigma_s' in paramdict.keys():\n",
    "        Sigma_s = paramdict['Sigma_s']\n",
    "    if 'Sigma_J' in paramdict.keys():\n",
    "        Sigma_J = paramdict['Sigma_J']\n",
    "\n",
    "    # Plot transition function\n",
    "    xtmp, ytmp = np.meshgrid(np.linspace(np.min(z[0,:]),np.max(z[0,:]),30),\n",
    "                             np.linspace(np.min(z[1,:]),np.max(z[1,:]),30))\n",
    "    xstar = np.concatenate([xtmp.flatten()[:,None], ytmp.flatten()[:,None]], axis=1).T\n",
    "\n",
    "    L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u=Sigma_u, sig_s=Sigma_s, sig_J = Sigma_J)\n",
    "    mu_star, sig_star, K_pred = fp_predict(xstar, L, targets, params)\n",
    "\n",
    "    #print(time_full_iter(pvec, y, D, Nz, Ns)[0])\n",
    "    \n",
    "    quiver_fig = plotly.figure_factory.create_quiver(np.squeeze(xstar[0,:]), \n",
    "                                                     np.squeeze(xstar[1,:]), \n",
    "                                                     np.squeeze(mu_star[0,:]-xstar[0,:]), \n",
    "                                                     np.squeeze(mu_star[1,:]-xstar[1,:]),\n",
    "                                                    scale=.25,\n",
    "                                                    arrow_scale=.4,)\n",
    "\n",
    "#     quiver_fig = plotly.figure_factory.create_quiver(np.squeeze(y_t1_rsh[0,:]), \n",
    "#                                                  np.squeeze(y_t1_rsh[1,:]), \n",
    "#                                                  np.squeeze(y_t_rsh[0,:]-y_t1_rsh[0,:]), \n",
    "#                                                  np.squeeze(y_t_rsh[1,:]-y_t1_rsh[1,:]),\n",
    "#                                                 scale=.25,\n",
    "#                                                 arrow_scale=.4,)\n",
    "    \n",
    "    # Add points to figure\n",
    "    \n",
    "    # Map Sigma_s values to the range 12-22\n",
    "    if Sigma_s.size>1:\n",
    "        FP_SIZE = -np.squeeze(np.log(Sigma_s))\n",
    "        FP_SIZE = FP_SIZE - np.min(FP_SIZE)\n",
    "        FP_SIZE = (FP_SIZE/np.max(FP_SIZE))*10. + 12.\n",
    "    else:\n",
    "        FP_SIZE = 12.\n",
    "    \n",
    "    # Inducing point locations\n",
    "    quiver_fig['data'].append(\n",
    "        plt_type.Scatter(x=np.atleast_1d(np.squeeze(z[0,:])), y=np.atleast_1d(np.squeeze(z[1,:])), \n",
    "                         mode='markers', name=\"Inducing loc\", marker=dict(size=10)))\n",
    "    \n",
    "    # Estimated fixed points\n",
    "    quiver_fig['data'].append(\n",
    "        plt_type.Scatter(x=np.atleast_1d(np.squeeze(s[0,:])), y=np.atleast_1d(np.squeeze(s[1,:])), \n",
    "                         mode='markers', name=\"Fixed loc\", marker=dict(size=FP_SIZE)))\n",
    "        \n",
    "    # Add nullclines    \n",
    "    if nullclines is not None:\n",
    "        quiver_fig['data'].append(plt_type.Scatter(x=nullclines['nullcline1'][0,:], y=nullclines['nullcline1'][1,:]))\n",
    "        quiver_fig['data'].append(plt_type.Scatter(x=nullclines['nullcline2'][0,:], y=nullclines['nullcline2'][1,:]))\n",
    "        \n",
    "    \n",
    "    # Add layout\n",
    "    quiver_fig['layout'] = plt_type.Layout(xaxis={'range': [-1.,8.]}, yaxis={'range': [-1.,8.]})\n",
    "    \n",
    "    \n",
    "    plt(quiver_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single experiment in decision making data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y\n",
    "y_means = np.stack([y[:30,:, :].mean(0), y[-30:,:, :].mean(0)])\n",
    "x_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "Nz = 36\n",
    "Ns = 5\n",
    "\n",
    "#######################################################\n",
    "# Initialise the parameters\n",
    "paramdict = init_params(y_train, D, Nz, Ns, grad_sigma=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add transformations for certain parameters \n",
    "# (Note that the parameter indices may be changed by transforms! (for cholesky repres of matrices, \"SquareMatrix\" type))\n",
    "transforms = OrderedDict()\n",
    "for par in ['Sigma_0_0', 'Sigma_u', 'Sigma_s', 'Sigma_J', 'lengthscales', 'Sigma_eps', 'Sigma_nu', 'kernel_variance']:\n",
    "    transforms[par] = {}\n",
    "    transforms[par]['type'] = \"Square\"\n",
    "\n",
    "# Create vectorised and transformed representation\n",
    "(init_paramvec, dict_ind, dict_shape) = params_to_vec(paramdict, transforms=transforms)\n",
    "\n",
    "#######################################################\n",
    "# Optimise only certain elements of paramvec (messy with indices)\n",
    "opt_params = np.arange(init_paramvec.shape[0])\n",
    "opt_params = np.delete(opt_params, np.hstack([dict_ind['C'], dict_ind['z']])) # All except the ones listed here\n",
    "cur_pvec = init_paramvec[opt_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# Add bounds for parameters \n",
    "bnds = list(((None, None),) * init_paramvec.shape[0])\n",
    "\n",
    "bnds_final = []\n",
    "for i in opt_params:\n",
    "    bnds_final.append(bnds[i])\n",
    "bnds = tuple(bnds_final)\n",
    "\n",
    "#######################################################\n",
    "# Add priors (to span at least the bounds)\n",
    "priors = []\n",
    "\n",
    "# Add prior to ensure inducing points are smooth\n",
    "cur_prior = {}\n",
    "cur_prior['type'] = \"InducingSmooth_and_DPP\"\n",
    "cur_prior['metadata'] = {}\n",
    "def unpack_dict_tmp(pdict):\n",
    "    kernelparams = {'lengthscales': pdict['lengthscales'], 'kernel_variance': pdict['kernel_variance']}\n",
    "    # Return the parameters we want in the required format (joint smoothness of (z-u) and s)\n",
    "    return (np.concatenate([pdict['u'], pdict['s']],axis=1), \n",
    "            np.concatenate([pdict['z'], pdict['s']],axis=1),\n",
    "            np.concatenate([pdict['Sigma_u'], pdict['Sigma_s']]),\n",
    "            kernelparams)\n",
    "cur_prior['metadata']['unpack_dict'] = unpack_dict_tmp\n",
    "cur_prior['metadata']['kernel_func'] = RBF\n",
    "cur_prior['metadata']['prior_weight_Smooth'] = 1e0\n",
    "cur_prior['metadata']['prior_weight_DPP'] = 1e0\n",
    "priors.append(cur_prior)\n",
    "\n",
    "\n",
    "# Prepare the optimisation\n",
    "f_objective = lambda pvec_partial: (time_full_iter(replace_params(pvec_partial, opt_params, init_paramvec), \n",
    "                                            y_train, dict_ind, dict_shape, \n",
    "                                            transforms=transforms,\n",
    "                                            priors=priors)[0])\n",
    "f_minibatch = lambda pvec_partial: (minibatch_iter(replace_params(pvec_partial, opt_params, init_paramvec), \n",
    "                                            y_train, dict_ind, dict_shape, \n",
    "                                            transforms=transforms,\n",
    "                                            priors=priors,\n",
    "                                            minibatch_size=3)[0])\n",
    "f_minibatch_with_grad = grad(f_minibatch, argnum=0)\n",
    "\n",
    "f_objective_with_grad = grad(f_objective, argnum=0)\n",
    "\n",
    "\n",
    "\n",
    "# By iterating minimize within a for cycle, we can save all intermediate results and set ending times\n",
    "start_time = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "save_fname_params = \"\"\n",
    "save_fname = \"Experiment_machens_results/machens_\" + simParams['curData'] +\"_\" + start_time + save_fname_params + \".pkl\"\n",
    "init_time = time.time()\n",
    "max_time = 2.0*3600 # Maximum iteration time in seconds, break if reached\n",
    "all_results = []\n",
    "\n",
    "saving_callback_params = [save_fname, y_train, x_train, simParams,\n",
    "                     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "                     bnds, transforms]\n",
    "def saving_callback_func(cur_result, save_fname, y_train, x_train, simParams,\n",
    "                     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "                     bnds, transforms):\n",
    "    # Saving every 10 iterations\n",
    "    if ~np.mod(cur_result.iters[-1], 10):\n",
    "        pickle.dump([y_train, x_train,\n",
    "                     cur_result, simParams,\n",
    "                     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "                     bnds, transforms], open(save_fname, 'wb'))\n",
    "        \n",
    "\n",
    "\n",
    "# for it in range(100):\n",
    "#     result = scipy.optimize.minimize(f_objective_with_grad, cur_pvec, jac=True, method='L-BFGS-B', bounds=bnds, callback=None,\n",
    "#                           options={'maxiter':10, 'disp':True})\n",
    "for it in range(5):\n",
    "    np.random.seed(1234+it)\n",
    "    result = adamOptimize(f_objective, f_minibatch_with_grad, cur_pvec,\n",
    "                      options={'obj_eval_iters': 5, 'maxiter':200, 'disp':True},\n",
    "                         callback_func=saving_callback_func, callback_params=saving_callback_params)\n",
    "    all_results.append(result)\n",
    "    # Save the results\n",
    "    with open(save_fname, 'wb') as f:\n",
    "        pickle.dump([y_train, x_train,\n",
    "                     all_results, simParams,\n",
    "                     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "                     bnds, transforms], f)\n",
    "    cur_pvec = result.x\n",
    "    cur_time = time.time()\n",
    "\n",
    "    machens_callback_plot_external(cur_pvec, \n",
    "                              opt_params, init_paramvec, transforms, dict_ind, dict_shape\n",
    "                             )\n",
    "\n",
    "    # Exit if maximum time is reached\n",
    "    if ((cur_time - init_time) > max_time):\n",
    "        print([\"Maximum iteration time reached at iter\", it])\n",
    "        break\n",
    "\n",
    "#         if len(all_results)>=2:\n",
    "#             if (all_results[-1].fun - all_results[-2].fun) >= (-1e-2*num_trials):\n",
    "#                 print([\"Update did not improve objective function, stopping\"])\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machens_callback_plot_external(cur_pvec, \n",
    "                              opt_params, init_paramvec, transforms, dict_ind, dict_shape\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.write(1, \"bla\".encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "final_paramdict = vec_to_params(final_paramvec, dict_ind, dict_shape, transforms)\n",
    "\n",
    "# Unpack the usual parameters\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, tmp1212, tmp1234)  = \\\n",
    "    final_paramdict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_pvec = result.x\n",
    "cur_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machens_callback_plot(init_paramvec[opt_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machens_callback_plot(cur_pvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramvec = init_paramvec\n",
    "paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "\n",
    "# Unpack the usual parameters\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, Sigma_s, Sigma_J)  = \\\n",
    "    paramdict.values()\n",
    "    \n",
    "np.set_printoptions(precision=8)\n",
    "print(np.concatenate([s.T, Sigma_s], axis=1))\n",
    "\n",
    "paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "paramdict = vec_to_params(paramvec, dict_ind, dict_shape, transforms)\n",
    "\n",
    "# Unpack the usual parameters\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J, Sigma_s, Sigma_J)  = \\\n",
    "    paramdict.values()\n",
    "    \n",
    "np.set_printoptions(precision=8)\n",
    "print(np.concatenate([s.T, Sigma_s], axis=1))\n",
    "\n",
    "paramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(J[0,:,:])[1][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting individual trajectories and estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"GPDM_direct_fixedpoints.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot smoothed trajectories\n",
    "obj, x_t1, x_t, sig_t1, sig_t, negll_all, _ = time_full_iter(replace_params(all_results[-1].x, opt_params, init_paramvec), \n",
    "                                               y, dict_ind, dict_shape, transforms=transforms, ret_smoothed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmin(negll_all), negll_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt([plt_type.Histogram(x=np.squeeze(negll_all.flatten()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_trajectories(y,x,\n",
    "                      x_t1, x_t, sig_t1, sig_t,\n",
    "                      v,\n",
    "                      ax_pixel = 700,\n",
    "                      ax_range = (-0.5,6.5),\n",
    "                      user_scale = 1.0,\n",
    "                      marker_op=0.3):\n",
    "\n",
    "    \n",
    "    # Use proper scaling of the markers to represent standard deviation (or a fraction of it based on user-scale)\n",
    "    std_scale = user_scale * (ax_pixel/(ax_range[1]-ax_range[0]))\n",
    "    \n",
    "    plots_by_run = [] \n",
    "    plots_by_run.append(\n",
    "        plt_type.Scatter(x=np.squeeze(y[0,:,v]), \n",
    "                      y=np.squeeze(y[1,:,v]), \n",
    "                      mode='lines',\n",
    "                        name='y')\n",
    "    )\n",
    "    \n",
    "#     plots_by_run.append(\n",
    "#         plt_type.Scatter(x=np.squeeze(x[0,:,v]), \n",
    "#                       y=np.squeeze(x[1,:,v]), \n",
    "#                       mode='lines')\n",
    "#     )\n",
    "\n",
    "    plots_by_run.append(\n",
    "        plt_type.Scatter(x=np.squeeze(x_t[0,:,v]), \n",
    "                      y=np.squeeze(x_t[1,:,v]), \n",
    "                      mode='lines',\n",
    "                        name ='x_t')\n",
    "    )\n",
    "    \n",
    "    plots_by_run.append(\n",
    "        plt_type.Scatter(x=np.squeeze(x_t1[0,:,v]), \n",
    "                      y=np.squeeze(x_t1[1,:,v]), \n",
    "                      mode='markers',\n",
    "                    marker=dict(size=np.squeeze(std_scale*np.sqrt(np.mean(sig_t1[[0,3],:,v], axis=0))),\n",
    "                               opacity=marker_op),\n",
    "                        name = 'x_t1')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    layout1 = plt_type.Layout(\n",
    "        width=ax_pixel, \n",
    "        height=ax_pixel,\n",
    "        xaxis=dict(range=ax_range),\n",
    "        yaxis=dict(range=ax_range)\n",
    "    )\n",
    "    \n",
    "    plt(plt_type.Figure(data=plots_by_run, layout=layout1))\n",
    "\n",
    "\n",
    "    layout2 = plt_type.Layout(\n",
    "        height=ax_pixel,\n",
    "        yaxis=dict(range=ax_range)\n",
    "    )\n",
    "    \n",
    "\n",
    "    plt_data = [\n",
    "        plt_type.Scatter(x=np.squeeze(np.arange(y.shape[1])), \n",
    "                      y=np.squeeze(y[0,:,v]), \n",
    "                      mode='lines', name='y_0'),\n",
    "        \n",
    "        plt_type.Scatter(x=np.squeeze(np.arange(y.shape[1])), \n",
    "                      y=np.squeeze(y[1,:,v]), \n",
    "                      mode='lines', name='y_1'),\n",
    "    \n",
    "                plt_type.Scatter(x=np.squeeze(np.arange(y.shape[1])), \n",
    "                      y=np.squeeze(x_t[0,:,v]), \n",
    "                      mode='lines', name='xt_0'),\n",
    "                plt_type.Scatter(x=np.squeeze(np.arange(y.shape[1])), \n",
    "                      y=np.squeeze(x_t[1,:,v]), \n",
    "                      mode='lines', name='xt_1'),\n",
    "            \n",
    "            plt_type.Scatter(x=np.squeeze(np.arange(y.shape[1])), \n",
    "                      y=np.squeeze(x_t1[0,:,v]), \n",
    "                      mode='lines+markers', name='xt1_0',\n",
    "                            marker=dict(size=np.squeeze(std_scale*np.sqrt(sig_t1[0,:,v])),\n",
    "                                       opacity=marker_op)),\n",
    "        \n",
    "                        plt_type.Scatter(x=np.squeeze(np.arange(y.shape[1])), \n",
    "                      y=np.squeeze(x_t1[1,:,v]), \n",
    "                      mode='lines+markers', name='xt1_1',\n",
    "                            marker=dict(size=np.squeeze(std_scale*np.sqrt(sig_t1[-1,:,v])),\n",
    "                                        opacity=marker_op)\n",
    "                                        )\n",
    "    ]\n",
    "    \n",
    "    plt(plt_type.Figure(data=plt_data, layout=layout2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(y_means,[],\n",
    "                      x_t1, x_t, sig_t1, sig_t,\n",
    "                      1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcur = 5; v = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_t1[:,tcur:(tcur+1),v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_t1[:,tcur:(tcur+1),v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_t_t1 = np.reshape(sig_t1[:,tcur:(tcur+1),v],(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_t_t1 = x_t1[:,tcur:(tcur+1),v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = y[:,tcur:(tcur+1),v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_gauss = np.dot(C, mu_t_t1) \n",
    "var_gauss = np.dot(np.dot(C, Sigma_t_t1), C.T) + np.diag(Sigma_nu.flatten())\n",
    "\n",
    "# Check for condition number, if high compared to expected precision in y (Sigma_nu), use pseudo-inverse and pseudo-determinant\n",
    "prec = np.mean(np.log(1./Sigma_nu))\n",
    "prec = prec-1\n",
    "if np.log(np.linalg.cond(var_gauss)) > prec:\n",
    "    inv_var_gauss = np.linalg.pinv(var_gauss, rcond=(1./prec))\n",
    "    ldet = np.abs(np.linalg.eigvals(var_gauss))\n",
    "    ldet = np.sum(np.log(ldet[np.max(ldet)>(1./prec)]))\n",
    "else:    \n",
    "    inv_var_gauss = np.linalg.inv(var_gauss)\n",
    "    ldet = np.linalg.slogdet(var_gauss)[1]\n",
    "\n",
    "D = mu_t_t1.shape[1]*1.0\n",
    "\n",
    "log_marg_ll = (\n",
    "    - 1.0* D/2.0*np.log(2*np.pi)\n",
    "    - 1.0/2.0*ldet\n",
    "    - 1.0/2.0*np.dot(np.dot( (y_t.T - mu_gauss.T), inv_var_gauss), (y_t - mu_gauss) )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(np.linalg.eigvals(var_gauss))/np.max(np.abs(np.linalg.eigvals(var_gauss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = np.mean(np.log(1./Sigma_nu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(np.linalg.cond(var_gauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(np.linalg.cond(var_gauss)) > prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_marg_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_var_gauss = np.linalg.inv(var_gauss)\n",
    "ldet = np.linalg.slogdet(var_gauss)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.slogdet(var_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 1.0/2.0*np.dot(np.dot( (y_t.T - mu_gauss.T), inv_var_gauss), (y_t - mu_gauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldet = np.linalg.svd(var_gauss, compute_uv=False)\n",
    "ldet = np.sum(np.log(ldet[ldet>(1./prec)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.svd(var_gauss, compute_uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldet = np.linalg.svd(var_gauss, compute_uv=False)\n",
    "ldet = np.sum(np.log(ldet[ldet>(1./prec)]))\n",
    "- 1.0/2.0*ldet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_var_gaussa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t - mu_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.svd(var_gauss, compute_uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldet[ldet>(1./prec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_marg_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlog_marg_ll(mu_t_t1, Sigma_t_t1, C, Sigma_nu, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(Sigma_nu.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.dot(C, Sigma_t_t1), C.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(Sigma_t_t1 + np.diag(Sigma_nu.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_params(all_results[-1].x, opt_params, init_paramvec)[dict_ind['Sigma_eps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = np.mean(np.log(1./Sigma_nu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = np.mean(np.log(1./Sigma_nu))\n",
    "np.log(np.linalg.cond(var_gauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_var_gauss = np.linalg.inv(var_gauss)\n",
    "ldet = np.linalg.slogdet(var_gauss)[1]\n",
    "print(inv_var_gauss)\n",
    "print(ldet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.svd(var_gauss, compute_uv=False)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(var_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(var_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1./prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_var_gauss = np.linalg.pinv(var_gauss, rcond=(1./prec))\n",
    "ldet = np.abs(np.linalg.eigvals(var_gauss))\n",
    "ldet = np.sum(np.log(ldet[(ldet/np.max(ldet))>(1./prec)]))\n",
    "print(inv_var_gauss)\n",
    "print(ldet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_marg_ll = (\n",
    "    - 1.0* D/2.0*np.log(2*np.pi)\n",
    "    - 1.0/2.0*np.linalg.slogdet(var_gauss)[1] \n",
    "    - 1.0/2.0*np.dot(np.dot( (y_t.T - mu_gauss.T), inv_var_gauss), (y_t - mu_gauss) )  \n",
    ")\n",
    "print log_marg_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_gauss = np.dot(C, mu_t_t1) \n",
    "var_gauss = np.dot(np.dot(C, Sigma_t_t1), C.T) + np.diag(Sigma_nu.flatten())\n",
    "\n",
    "inv_var_gauss = np.linalg.inv(var_gauss)\n",
    "\n",
    "D = mu_t_t1.shape[1]*1.0\n",
    "\n",
    "log_marg_ll = (\n",
    "    - 1.0* D/2.0*np.log(2*np.pi)\n",
    "    - 1.0/2.0*np.linalg.slogdet(var_gauss)[1] \n",
    "    - 1.0/2.0*np.dot(np.dot( (y_t.T - mu_gauss.T), inv_var_gauss), (y_t - mu_gauss) )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_gauss = np.dot(np.dot(C, Sigma_t_t1), C.T) + np.diag(Sigma_nu.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(var_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(np.linalg.slogdet(var_gauss)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_t_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_t.T - mu_gauss.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.slogdet(var_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_var = np.dot(C, Sigma_t_t1)\n",
    "proj_inv = np.linalg.inv(np.dot(proj_var, C.T) + np.diag(Sigma_nu.flatten()))\n",
    "proj_back = np.dot(Sigma_t_t1, np.dot(C.T, proj_inv))\n",
    "\n",
    "mu_t_t = mu_t_t1 + np.dot(proj_back, (y_t - np.dot(C, mu_t_t1)))\n",
    "\n",
    "Sigma_t_t = Sigma_t_t1 - np.dot(proj_back, proj_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t - np.dot(C, mu_t_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(Sigma_nu.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(np.dot(proj_var, C.T)+ np.diag(Sigma_nu.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_t_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machens_callback_plot(cur_pvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('machens_20171206T072804.pkl', 'r') as f:\n",
    "    [y, all_results, init_paramvec, dict_ind, dict_shape, opt_params, bnds, log_transformed] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_pvec = all_results[-1].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machens_callback_plot(init_paramvec[opt_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_results)):\n",
    "    machens_callback_plot(all_results[i].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_pvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Experiment_machens_results/machens_decision_20181205T144909.pkl', 'rb') as f:\n",
    "    [y_train, x_train,\n",
    "     cur_result, simParams,\n",
    "     init_paramvec, dict_ind, dict_shape, opt_params, \n",
    "     bnds, transforms] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Load held out data\n",
    "maxTrials = len(curData['data']['rt'])\n",
    "testSimParams = copy.deepcopy(simParams)\n",
    "testSimParams['Ntrain'] = maxTrials\n",
    "\n",
    "y = createData(curData, testSimParams, batchnum=0)\n",
    "\n",
    "y_test = y[:,:,simParams['Ntrain']:]\n",
    "y = y[:,:,:simParams['Ntrain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load held out data\n",
    "# np.random.seed(simParams['seed'])\n",
    "# x_test = tmp['y_python'][:,0::simParams['delta_t'],simParams['Ntrain']:]*simParams['rescale'] # Rescaling to reduce numerical instability\n",
    "# y_test = x_test + np.sqrt(simParams['Sigma_y'])*np.random.randn(*x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One step ahead given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear AR(1) on y as excitation\n",
    "Dy = y_test.shape[0]\n",
    "y_t = y[:,1:,:]\n",
    "y_t1 = y[:,:-1,:]\n",
    "\n",
    "y_t_rsh = np.reshape(y_t, (y.shape[0], -1)).T\n",
    "y_t1_rsh = np.reshape(y_t1, (y.shape[0], -1)).T\n",
    "\n",
    "# Get least squares linear regression weights from original dataset\n",
    "weights_AR = np.dot(np.dot(np.linalg.inv(np.dot(y_t1_rsh.T, y_t1_rsh)), y_t1_rsh.T), y_t_rsh)\n",
    "\n",
    "# Get optimal linear estimate of y_tt given the y_t-1 vector\n",
    "y_test_t1_rsh = np.reshape(y_test[:,:-1,:], (y.shape[0], -1)).T\n",
    "y_test_t_hat = np.dot(y_test_t1_rsh, weights_AR)\n",
    "\n",
    "y_test_t_hat = np.reshape(y_test_t_hat.T, (y_test.shape[0], y_test.shape[1]-1, y_test.shape[2]))\n",
    "y_test_t_hat = np.concatenate([np.zeros((y_test.shape[0],1,y_test.shape[2])), y_test_t_hat], axis=1)\n",
    "y_test_t_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the smoothing on this data, then check for RMSE against true x\n",
    "# res = [obj, x_t1, x_t, sig_t1, sig_t, negll_all]\n",
    "res_heldout_init = time_full_iter(init_paramvec, \n",
    "                                               y_test, dict_ind, dict_shape,  transforms=transforms, ret_smoothed=True)\n",
    "res_heldout_final = time_full_iter(replace_params(cur_result[-1].x, opt_params, init_paramvec), \n",
    "                                               y_test, dict_ind, dict_shape, transforms=transforms, ret_smoothed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_heldout_init[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_heldout_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error values between x_t1 and true x\n",
    "RMSE_AR = np.sqrt(np.mean(np.mean((y_test_t_hat-x_test)**2, axis=2), axis=0))\n",
    "RMSE_final = np.sqrt(np.mean(np.mean((res_heldout_final[1]-x_test)**2, axis=2), axis=0))\n",
    "RMSE_init = np.sqrt(np.mean(np.mean((res_heldout_init[1]-x_test)**2, axis=2), axis=0))\n",
    "\n",
    "plt([\n",
    "        plt_type.Scatter(y=np.squeeze(RMSE_final), name='RMSE_final'),\n",
    "        plt_type.Scatter(y=np.squeeze(RMSE_init), name='RMSE_init'),\n",
    "        plt_type.Scatter(y=np.squeeze(RMSE_AR), name='RMSE_AR')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run forward given initial data point on each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get estimate of true variance (around fixed point)\n",
    "x_var_true = np.var(np.reshape(x[:,x.shape[1]-10:x.shape[1],:] - x[:,x.shape[1]-11:x.shape[1]-1,:], (x.shape[0], -1)).T, axis = 0)\n",
    "x_var_true = x_var_true[:,None]\n",
    "x_var_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from time_full_iter\n",
    "\n",
    "Dy = y.shape[0]\n",
    "T = y.shape[1]\n",
    "Ny = y.shape[2]\n",
    "    \n",
    "paramvec = replace_params(all_results[-1].x, opt_params, init_paramvec)\n",
    "paramdict_ind = dict_ind\n",
    "paramdict_shape = dict_shape\n",
    "\n",
    "# Transform the parameters back from log-space\n",
    "if log_transformed is not None:\n",
    "    inds = (np.arange(paramvec.shape[0]))\n",
    "    inds = np.setdiff1d(inds,log_transformed)\n",
    "    out = np.concatenate([np.exp(paramvec[log_transformed]), paramvec[inds]])\n",
    "    out = out[np.argsort(np.concatenate([log_transformed, inds]))]\n",
    "    paramvec = out\n",
    "\n",
    "\n",
    "# Unpack the usual parameters\n",
    "param_tuple = vec_to_params(paramvec, paramdict_ind, paramdict_shape)\n",
    "(Sigma_eps, mu_0_0, Sigma_0_0, C, Sigma_nu, z, u, Sigma_u, lengthscales, kernel_variance, s, J)  = \\\n",
    "    tuple(list(param_tuple)[:12])\n",
    "\n",
    "if np.any(np.isnan(lengthscales)):\n",
    "    set_trace()\n",
    "\n",
    "# Deal with the extra possible parameters\n",
    "Sigma_s = None; Sigma_J=None;\n",
    "if 'Sigma_s' in paramdict_ind.keys():\n",
    "    Sigma_s = np.reshape(paramvec[paramdict_ind['Sigma_s']], paramdict_shape['Sigma_s'])\n",
    "if 'Sigma_J' in paramdict_ind.keys():\n",
    "    Sigma_J = np.reshape(paramvec[paramdict_ind['Sigma_J']], paramdict_shape['Sigma_J'])\n",
    "\n",
    "L, targets, params = fp_get_static_K(eta=kernel_variance, lengthscales=lengthscales, z=z, u=u, s=s, J=J, \n",
    "                                         sig_eps=Sigma_eps, sig_u = Sigma_u, sig_s=Sigma_s, sig_J=Sigma_J)\n",
    "\n",
    "# Collect smoothed latent trajectories\n",
    "x_all_t1 = np.zeros(x_test.shape)\n",
    "sig_all_t1 = np.zeros((x_test.shape[0]**2, x_test.shape[1], x_test.shape[2]))\n",
    "\n",
    "for n in range(x_test.shape[2]):\n",
    "    mu_t1_t1 = mu_0_0\n",
    "    Sigma_t1_t1 = Sigma_0_0\n",
    "        \n",
    "    for t in range(x_test.shape[1]):\n",
    "        mu_t_t1, Sigma_t_t1 = update_t_t1(mu_t1_t1, Sigma_t1_t1, L, targets, kernel_variance, \n",
    "                                          Sigma_eps, z, u, lengthscales, s, J)\n",
    "        \n",
    "        x_all_t1[:,t,n] = mu_t_t1.flatten()            \n",
    "        sig_all_t1[:,t,n] = np.reshape(Sigma_t_t1,(1,-1))\n",
    "        \n",
    "        cur_nll_term, mu_t1_t1, Sigma_t1_t1 = update_t_t(mu_t_t1, Sigma_t_t1, C, Sigma_nu, y_test[:,t:(t+1),n])\n",
    "        #print [mu_t1_t1, y_test[:,t:(t+1),n]]\n",
    "        Sigma_t1_t1 = np.diag(Sigma_t1_t1)[:,None]\n",
    "        if t>2:\n",
    "            # Integrate the first Tau data points, but then no more data available (ignore mu_t1_t1, do we add noise?)\n",
    "            mu_t1_t1 = mu_t_t1 # + np.sqrt(Sigma_eps)*np.random.randn(*mu_t_t1.shape)\n",
    "\n",
    "        # Check \"condition number\" and add diag term to correct if needed            \n",
    "        if (np.min(Sigma_t1_t1)<1e-6):\n",
    "            Sigma_t1_t1 = Sigma_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(y_test,x_test,\n",
    "                      x_all_t1, x_all_t1, sig_all_t1, sig_all_t1,\n",
    "                      1, marker_op=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machens_callback_plot(all_results[-1].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
